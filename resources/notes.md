â¸»

ðŸ§¾ Full Line-by-Line Explanation

â¸»

ðŸ“Œ Step 1: Import Libraries

import pandas as pd
import numpy as np

	â€¢	pandas is used for handling data in table (DataFrame) format.
	â€¢	numpy is used for numerical operations (not directly used here but a common utility).

from sklearn.model_selection import train_test_split

	â€¢	Used to split your dataset into training and testing sets.

from sklearn.feature_extraction.text import TfidfVectorizer

	â€¢	Converts text into numbers using the TF-IDF method (explained below).

from sklearn.preprocessing import LabelEncoder

	â€¢	Converts text labels like "spam" and "ham" into numbers (e.g., 1 and 0).

from sklearn.linear_model import LogisticRegression

	â€¢	The ML model used here: Logistic Regression â€” good for binary classification (spam vs ham).

from sklearn.metrics import classification_report, confusion_matrix

	â€¢	Tools to evaluate your model: accuracy, precision, recall, F1-score, and confusion matrix.

import seaborn as sns
import matplotlib.pyplot as plt

	â€¢	Libraries to visualize the confusion matrix using a heatmap.

â¸»

ðŸ“Œ Step 2: Load Dataset

url = 'https://raw.githubusercontent.com/justmarkham/pycon-2016-tutorial/master/data/sms.tsv'
df = pd.read_csv(url, sep='\t', header=None, names=['label', 'message'])

	â€¢	Loads the dataset from a URL.
	â€¢	sep='\t' means the data is tab-separated (TSV format).
	â€¢	header=None tells pandas thereâ€™s no header row.
	â€¢	names=['label', 'message'] assigns column names manually.

print("Dataset Loaded Successfully!")
print(df.head())

	â€¢	Just confirming the dataset loaded correctly by printing the first 5 rows.

â¸»

ðŸ“Œ Step 3: Encode Labels

label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['label'])  # spam = 1, ham = 0

	â€¢	Converts "ham" and "spam" into 0 and 1 respectively so that the model can work with them.
	â€¢	Adds a new column label_encoded to the dataframe.

â¸»

ðŸ“Œ Step 4: Train-Test Split

X_train, X_test, y_train, y_test = train_test_split(df['message'], df['label_encoded'], test_size=0.2, random_state=42)

	â€¢	Splits the dataset into:
	â€¢	Training data: 80% â†’ used to train the model
	â€¢	Testing data: 20% â†’ used to test how well the model performs
	â€¢	random_state=42 ensures repeatability (same split every time you run it).

â¸»

ðŸ“Œ Step 5: Vectorize Text with TF-IDF

vectorizer = TfidfVectorizer(stop_words='english')
X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

What is TF-IDF?
	â€¢	TF = Term Frequency (how often a word appears in a document)
	â€¢	IDF = Inverse Document Frequency (how rare the word is across all documents)
	â€¢	It converts text messages into numerical vectors that represent how important each word is.
	â€¢	stop_words='english' removes common useless words like â€œtheâ€, â€œisâ€, â€œaâ€, etc.
	â€¢	fit_transform is used on training data to learn the vocabulary and convert.
	â€¢	transform is used on test data to apply the same transformation.

â¸»

ðŸ“Œ Step 6: Train Classifier

model = LogisticRegression()
model.fit(X_train_vec, y_train)

	â€¢	Creates and trains the Logistic Regression model on the vectorized training data.

â¸»

ðŸ“Œ Step 7: Evaluate Model

y_pred = model.predict(X_test_vec)

	â€¢	Uses the model to predict spam or ham for the test messages.

print("\nClassification Report:\n")
print(classification_report(y_test, y_pred, target_names=['Ham', 'Spam']))

	â€¢	Prints detailed model performance:
	â€¢	Precision: out of all predicted spam, how many were correct?
	â€¢	Recall: out of all actual spam, how many did we catch?
	â€¢	F1 Score: balance between precision and recall

â¸»

ðŸ“Œ Step 8: Confusion Matrix

cm = confusion_matrix(y_test, y_pred)

	â€¢	Creates a confusion matrix, which shows:
	â€¢	True Positives (Spam correctly identified)
	â€¢	True Negatives (Ham correctly identified)
	â€¢	False Positives (Ham marked as Spam)
	â€¢	False Negatives (Spam missed as Ham)

sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Ham', 'Spam'], yticklabels=['Ham', 'Spam'])
plt.title("Confusion Matrix")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

	â€¢	Plots the confusion matrix as a colored heatmap for easy visualization.

â¸»

ðŸ“Œ Step 9: Create Predict Function

def predict_spam(text):
    vec = vectorizer.transform([text])     # Convert new text into same vector format
    prediction = model.predict(vec)        # Predict using the trained model
    return "Spam" if prediction[0] == 1 else "Ham"

	â€¢	This is a helper function to classify any new message as â€œSpamâ€ or â€œHamâ€.
	â€¢	It first vectorizes the input message and then predicts using the trained model.

â¸»

ðŸ“Œ Step 10: Try Sample Predictions

samples = [
    "Congratulations! Youâ€™ve won a free iPhone. Click to claim now!",
    "Can we meet at 5 PM for coffee?",
    "URGENT! Your loan has been approved. Reply to get funds.",
    "I'll call you later tonight."
]

	â€¢	A list of sample messages to test the model.

print("\nSample Predictions:\n")
for msg in samples:
    print(f"> {msg} \nPrediction: {predict_spam(msg)}\n")

	â€¢	Loops through each message, runs it through the model, and prints whether itâ€™s spam or not.
